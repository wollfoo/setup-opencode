# TDD Workflow Process

## CRITICAL: Direct Code Writing with User Collaboration

**TDD agents write code directly, and main conversation coordinates the workflow.**

### How TDD Collaboration Works

**TDD Agents (red-tdd-tester, green-implementer):**
- Write test and implementation code directly using Write/Edit tools
- After user approval, RE-READ files to see actual final state (user may modify before accepting)
- Answer any QUESTION: comments user added
- Remove QUESTION: comments and continue
- Main conversation coordinates between agents

**Main Conversation Coordinates:**
- Launches appropriate TDD agent for Red/Green phase
- Monitors build and test state
- Ensures proper handoff between Red → Domain → Green
- Verifies quality gates before auto-commit
- User is involved via OpenCode's built-in approval for file changes

**User Participates:**
- Reviews file changes via OpenCode's built-in approval
- Can modify proposed code directly in IDE before accepting
- Adds `QUESTION:` comments inline for clarification
- Agent sees modifications when it re-reads file post-approval

### QUESTION: Comment Mechanism in Code

During TDD, user can add inline questions while reviewing file changes:

```rust
#[test]
fn test_payment_capture() {
    let amount = MonetaryAmount::new(100, Currency::USD);
    // QUESTION: Should we also test negative amounts here or in a separate test?
    assert_eq!(amount.value(), 100);
}
```

**CRITICAL:** After approval, agent RE-READS the file, sees the QUESTION comment, answers it, updates code accordingly, and removes the comment. Never leave QUESTION comments in committed code.

## ⚠️ CRITICAL: ONLY ADDRESS EXACT ERROR MESSAGES ⚠️

**THE GOLDEN RULE OF TDD:**

**ONLY IMPLEMENT WHAT IS CALLED FOR BY THE ACTUAL TEST FAILURE MESSAGE!**

Nothing—absolutely nothing—should ever address more than the **exact test
failure message** and no more!

### Agent Boundaries (NON-NEGOTIABLE)

**Red-TDD-Tester:**
- Fixes **TEST CODE ONLY**
- Responds to compilation errors **IN TEST FILES**
- Never touches production code
- **ONLY** addresses the exact compiler error in the test file
- If test needs production code, delegates to green-implementer

**Green Implementer:**
- Fixes **PRODUCTION CODE ONLY**
- Responds to test assertion failures
- Never touches test code
- **ONLY** addresses the exact test failure message or runtime error
- No validation/assertions unless tests demand them
- No "convenience methods" or extra API beyond what test calls

**Domain Modeler:**
- Runs **AFTER** each red and green step (NOT during)
- Reviews types for possible improvements
- Creates type signatures with `unimplemented!()` **ONLY**
- **NEVER IMPLEMENTS FUNCTION BODIES** - green-implementer does that
- **NEVER** adds validation/assertions unless tests demand them
- **NEVER** adds smart behavior proactively
- Only suggests type changes, does not anticipate future needs

### Dead Code Policy (ABSOLUTE)

**IF NOTHING IS USING IT, REMOVE IT!**

**DEAD CODE = DELETE, NOT IMPLEMENT**

- Unused fields? **DELETE THEM**
- Unused methods? **DELETE THEM**
- Unused structs/types? **DELETE THEM**
- Compiler warns about dead code? **DELETE IT, DON'T IMPLEMENT IT**
- Dead code means the test doesn't need it yet
- When tests need it, they will fail and demand it
- **NEVER** implement something just to make warnings go away
- **NEVER** add usage just to justify keeping it

### No Anticipatory Implementation

**ONLY IMPLEMENT WHAT THE TEST ERROR DEMANDS - NOTHING MORE!**

**FORBIDDEN:**
- ❌ Adding "convenience methods" not called by tests
- ❌ Implementing validation not required by failing tests
- ❌ Adding fields/methods because "we might need them"
- ❌ Any API that tests don't actually call
- ❌ Extra helper functions not demanded by test
- ❌ Smart behavior beyond exact test requirements
- ❌ Anticipating future needs based on domain knowledge

**REQUIRED:**
- ✅ Only respond to actual compiler errors
- ✅ Only respond to actual test failures
- ✅ Delete unused code immediately
- ✅ Let tests drive every single feature
- ✅ Implement ONLY what makes THIS test pass
- ✅ Stop immediately after test passes

## CRITICAL: Dependency Management

**BEFORE writing tests that require external dependencies:**

**MANDATORY**: Read and follow **DEPENDENCY_MANAGEMENT.md** process file

**NEVER directly edit** Cargo.toml, pyproject.toml, package.json, or any dependency file.

**ALWAYS call** the dependency-management agent when you need external dependencies.

### Dependency Resolution During TDD

**Trigger**: When TDD agents encounter missing dependencies during Red or Green phases

**Process**:
1. **Pause TDD Cycle**: Temporarily halt Red → Domain → Green process
2. **Call dependency-management agent**: Provide dependency name, purpose, required features
3. **Wait for Resolution**: dependency-management agent adds dependency and commits separately
4. **Resume TDD**: Continue with Red → Domain → Green cycle using new dependency

See DEPENDENCY_MANAGEMENT.md for complete protocol and examples.

## Outside-In TDD with Hierarchical Chained PRs

### CRITICAL TDD COMPLETION RULES

- **Project MUST compile cleanly before any TDD round is considered complete**
- **ALL tests MUST pass before any TDD round is considered complete**
- **NEVER write new tests when build is failing OR any test is failing**
- **NEVER exit TDD cycle with failing build or failing tests**

### Hierarchical PR Structure

```
main
└── PR #1: Integration Test for Feature X
    ├── PR #2: Unit Test for Component A (chains off PR #1)
    │   └── PR #3: Unit Test for Helper Function (chains off PR #2)
    └── PR #4: Unit Test for Component B (chains off PR #1)
```

## Outside-In TDD Process

### CRITICAL: Red Agent Constraints

**Red-TDD-Tester ONLY writes test code:**
- Reference type names in test (e.g., `Deposit`, `AccountDeposited`, `StreamId`)
- DO NOT create type definitions
- DO NOT stub types
- Let compilation fail
- Domain modeling agent handles compiler errors

**Example of Correct Red Phase:**
```rust
// Red agent writes THIS (references types, doesn't define them):
#[test]
fn test_deposit_command() {
    let store = InMemoryEventStore::new();
    let cmd = Deposit {
        account_id: StreamId::new("account-123").unwrap(),
        amount: 100
    };
    let result = execute(cmd, &store).await;
    assert!(result.is_ok());
}
// Compiler fails: "StreamId not found", "Deposit not found", etc.
// Domain modeler creates minimal stubs to make it compile
```

### The 5-Whys Decision Tree: When to Drill Down vs Implement

When a test fails, apply this decision tree:

```
Test Fails
  ↓
What type of failure?
  ├─ Compiler Error → Domain modeling agent reviews and either:
  │                   1. Creates minimal stub types (just enough to compile), OR
  │                   2. Points Red agent to existing types that should be used
  └─ Assertion/Runtime Failure
       ↓
  Is it OBVIOUS what code needs to change?
       ├─ YES: Make the single clear change (green-implementer)
       └─ NO: Drill down (Ask "Why?" and refine/write lower-level test)
              ↓
         Mark parent test #[ignore = "working on: child_test_name"]
              ↓
         Lower-level test fails
              ↓
         Repeat decision tree...
              ↓
         Child test passes → Remove parent ignore
              ↓
         Work back up test hierarchy
              ↓
         Parent test progresses/passes
```

**Key Distinctions:**

**Compiler Errors Always Drive Type Creation (by Domain Modeler):**
- Missing type/function: Domain agent creates minimal structure
- Missing field: Domain agent adds field
- Type mismatch: Domain agent refines types OR tells Red to use existing types
- **Red agent NEVER creates types** - only references them in tests
- **NEVER speculate** - only create what compiler demands

**Assertion Failures Often Need Drill-Down:**
- Multiple possible causes → NOT obvious → Drill down
- Unclear which component failed → NOT obvious → Drill down
- High-level behavior failure → NOT obvious → Drill down
- Single clear fix needed → Obvious → Implement

**Examples of "Obvious" Changes:**
- Test expects "Hello" but gets empty string → Add return "Hello"
- Test expects status=200 but gets 404 → Change response code
- Single assertion, single line fix → Obvious

**Examples of "Not Obvious" (Need Drill-Down):**
- Integration test: "message history not displayed" → Why? Multiple possible causes
- Unit test: "authentication failed" → Why? Could be credentials, network, config, etc.
- Any failure where you ask "which part is wrong?" → Not obvious

**The Drill-Down Process with Skip Protocol:**

1. **Mark parent test ignored** - `#[ignore = "working on: test_specific_component"]` to allow commits while drilling down
2. **Refine current test setup** - Ensure Given/Arrange properly creates the scenario (does test provide required inputs?), OR
3. **Write lower-level test** - Create unit test focusing on suspected component
4. **Let compiler drive types** - Setup/lower-level test won't compile → Types emerge
5. **Implement when obvious** - Make minimal change to pass lower-level test
6. **Remove parent ignore** - Once child test passes, unskip parent test
7. **Work back up** - Verify parent test progresses further
8. **Repeat** until integration test passes

**Skip Annotation Requirements:**
- Must reference specific child test: `#[ignore = "working on: test_message_rendering"]`
- Provides metadata trail showing dependency hierarchy
- Enables commits even with parent test incomplete
- **MANDATORY**: Remove ignore before considering parent test complete

## Red-Green-Refactor Cycle

**CRITICAL: Always Commit Before Refactoring**

The TDD cycle is **Red → Green → Refactor**, not just Red → Green:

1. **Red**: Write failing test
2. **Green**: Make test pass with minimal implementation
3. **✅ COMMIT working green state** ← MANDATORY before refactoring
4. **Refactor**: Extract duplication, improve design
5. **✅ COMMIT refactored code** after verifying tests still pass

**Why Commit First:**
- Creates safe rollback point if refactoring goes wrong
- Separates "make it work" from "make it better" in git history
- Allows easy revert to working state without losing progress
- Follows Kent Beck's original TDD methodology

**Example:**
```bash
# After test passes
git commit -m "Implement checkpoint resumption - test passes"

# Now safe to refactor
# Extract repeated logic, improve names, etc.

# After refactoring
pytest  # Verify tests still pass
git commit -m "Refactor checkpoint filter into helper function"
```

### 1. Integration Test PR

Create feature branch, write/refine integration test:
- Run test → hits first failure (unimplemented!() or assertion)
- Create PR #1 (stays open throughout feature development)

### 2. Drill Down with Skip Protocol

- Identify appropriate test scope for the failure
- **Create new branch** chaining off current branch
- **IMMEDIATELY mark parent test as skipped** with comment
- Write unit test at appropriate level
- Domain review → Can types prevent this?
- Green implementation if needed
- **Post-Implementation Domain Review**: Check for primitive obsession and type misuse
- Auto-commit when passing

### 3. Unskip Protocol Before Merge

- **MANDATORY**: Must unskip parent test before merging
- Verify parent test now progresses further (or passes)
- Create PR for review
- Merge child PR back to parent branch

### 4. Enhanced TDD Cycle per PR

#### Red-TDD-Tester: Write/refine failing test (ONE ASSERTION)

- **PREREQUISITE**: Project must compile cleanly and all other tests must pass
- **TEST WORKFLOW FUNCTIONS**: Focus on testing exported workflow functions from lib.rs, not pre-conceived type structures
- **ASSUME CODE EXISTS**: Write tests assuming the workflow functions you want exist, let compiler drive type creation
- **Property Testing**: Use property testing for domain type boundaries
- **Mutation Testing**: Add mutation score verification (≥80% for new code)

**MANDATORY: Unit Tests for ALL Functions**

Before ANY function implementation:
1. **Write unit tests for the specific function** covering:
   - Happy path with valid inputs
   - Edge cases (empty, boundary values, invalid inputs)
   - Property tests for domain invariants
   - Total function verification (no unexpected panics)

2. **Test BEFORE implementing** - Never implement without tests
3. **One assertion per test** - Keep tests focused
4. **Test domain constructors** - All constructors/builders need dedicated unit tests before implementation

#### Domain Modeling Agent: Review test

**Question**: "Can type system prevent this failure?"

- **CRITICAL**: Create MINIMAL nominal types only - no speculative structure!
- If YES: Create minimal nominal types (empty structs/enums) to replace primitives
- If NO: Proceed to Green Implementer
- **NEVER** add fields/methods without test demanding them

#### Green Implementer: Make minimal change to pass ONE assertion

- **BUILD VERIFICATION**: MUST verify project compiles cleanly after changes
- **TEST VERIFICATION**: MUST verify ALL tests pass after changes

#### Post-Implementation Domain Review

Domain modeler checks implementation:
- Check for primitive obsession (using primitives where nominal domain types should exist)
- Verify correct use of existing domain types
- **Check for over-implementation**: Ensure no speculative fields/methods added beyond test needs
- If violations found: Create minimal nominal types → Restart current PR's TDD cycle

#### Auto-Commit Integration

ONLY when project compiles cleanly AND all tests pass AND domain review approves:
- Green implementer calls source-control agent
- Auto-commit with descriptive message including test that passed
- Auto-push to remote branch
- **MANDATORY**: Main coordinator MUST verify commit success independently

### 5. TDD Round Completion

Round complete ONLY when:
- Project compiles without errors or warnings
- ALL tests pass (no failing, no skipped tests remain)
- All temporary test skips have been resolved
- Domain review approves implementation
- **MANDATORY**: Main coordinator has personally verified ALL above conditions

## MANDATORY VERIFICATION PROTOCOL

**MAIN COORDINATOR AGENT MUST PERSONALLY VERIFY EVERY TDD ROUND COMPLETION**

### After EVERY Green Implementer and Source Control Agent Report

1. **NEVER TRUST AGENT REPORTS** - Always verify independently
2. **BUILD VERIFICATION**: Run `mcp__cargo__cargo_check` or `mcp__cargo__cargo_test` personally
3. **TEST VERIFICATION**: Confirm ALL tests pass by running tests yourself
4. **COMMIT VERIFICATION**: Run git status via Bash tool to verify repository state
5. **CODE VERIFICATION**: Read actual implementation files to confirm changes

### TDD Round Completion Checklist (MANDATORY)

**✅ BEFORE marking ANY TDD round "complete":**
- [ ] **Personal build verification** - Run cargo test/check yourself
- [ ] **Personal test verification** - See "X passed; 0 failed" output yourself
- [ ] **Personal commit verification** - See "is_clean": true in git status yourself
- [ ] **Personal code verification** - Read the actual implementation yourself
- [ ] **Mutation testing verification** - Verify ≥80% mutation score for new code
- [ ] **Cognitive load verification** - Verify TRACE analysis passes before PR creation

### Violation Consequences

**If you proceed to next TDD round without personal verification:**
1. **IMMEDIATELY STOP** all work
2. **UNDO** any premature next-round work
3. **COMPLETE** the incomplete round properly
4. **UPDATE** system prompt to prevent recurrence

### Zero Exception Rule

**NEVER, UNDER ANY CIRCUMSTANCES, proceed to the next Red phase without:**
- Personal verification of build success
- Personal verification of all tests passing
- Personal verification of successful commit
- Personal confirmation repository is clean
- Personal verification of mutation testing compliance
- Personal verification of cognitive load compliance (for PR finalization)

**"Trust but verify" is WRONG - it should be "Never trust, always verify"**
