---
trigger: always_on
---
---
type: capability_prompt
scope: project
priority: critical
activation: always_on
category: advanced_reasoning_layer4

---

# 18a. Layer 4 ‚Äî Meta-Reasoning Protocol

## üìã Overview

**Layer 4 (Meta-Reasoning)** (suy lu·∫≠n si√™u nh·∫≠n th·ª©c) provides **self-reflection** (t·ª± ph·∫£n bi·ªán), **bias detection** (ph√°t hi·ªán thi√™n v·ªã), v√† **hallucination mitigation** (gi·∫£m ·∫£o gi√°c) cho high-stakes decisions.

**Philosophy**: "Verify Before Trust" ‚Äî Cross-check m·ªçi k·∫øt lu·∫≠n tr∆∞·ªõc khi output

**Part of**: Advanced Reasoning System (5 layers: Surface ‚Üí Intermediate ‚Üí Deep ‚Üí Meta ‚Üí Expert)

---

## Activation Triggers

```typescript
AUTO_ACTIVATE_LAYER_4:
  IF query.stakes === 'high' 
     OR confidence_during_reasoning < 0.6
     OR backward_pass_fails
     OR top_hypotheses_tied (within 0.1 confidence)
     OR hallucination_risk_detected:
    ‚Üí ESCALATE to Layer 4
```

**Examples**:
- "Should we invest $1M in this technology?" (high stakes)
- Confidence drops t·ª´ 0.9 ‚Üí 0.55 mid-process (uncertainty spike)
- Backward pass finds missing critical evidence (evidence gaps)
- Top 2 choices tied: 0.78 vs 0.79 (hypothesis conflict)

---

## Core Components

### 1. Self-Reflection Module

**Checklist**:

```yaml
reasoning_quality:
  questions:
    - "C√≥ logic leaps kh√¥ng ƒë∆∞·ª£c justify kh√¥ng?"
    - "Evidence c√≥ ƒë·ªß m·∫°nh support conclusion?"
    - "C√≥ xem x√©t counterarguments ch∆∞a?"
    - "Assumptions c√≥ explicit v√† reasonable?"
  scoring: 
    scale: 0-10
    threshold: "‚â•7 to pass"

bias_detection:
  check_for:
    - confirmation_bias: "Ch·ªâ t√¨m evidence supporting"
    - availability_bias: "D·ª±a qu√° nhi·ªÅu v√†o recent examples"
    - anchoring_bias: "B·ªã ·∫£nh h∆∞·ªüng initial information"
    - recency_bias: "∆Øu ti√™n information g·∫ßn ƒë√¢y"
  action: 
    - flag_if_detected: true
    - document_mitigation: "How addressed"

confidence_calibration:
  method: "Compare confidence vs historical accuracy"
  adjustment:
    - if_overconfident: "Reduce by 10-15%"
    - if_underconfident: "Increase by 5-10%"
  output: "Calibrated confidence (0-1)"
```

**Output Example**:

```markdown
## Layer 4 Meta-Reflection

**Reasoning Quality**: 8.5/10
- ‚úÖ All steps justified
- ‚úÖ Counterarguments considered
- ‚ö†Ô∏è Could explore edge case X deeper

**Biases Detected**:
- ‚ö†Ô∏è Confirmation bias: Mitigated by lateral pass
- ‚úÖ No availability/anchoring bias

**Confidence Calibration**:
- Raw: 0.82
- Historical accuracy at 0.82: 0.75 (overconfident)
- **Calibrated**: 0.72
```

---

### 2. Hallucination Detection

**Foundation**: Anthropic Research "Tracing Thoughts" (2024)

**Key Findings**:
- Hallucinations khi "known entity" feature activates cho unknown
- Refusal circuit b·ªã suppress incorrectly
- Model recognizes name nh∆∞ng kh√¥ng bi·∫øt details ‚Üí confabulation

**Detection Algorithm**:

```python
def detect_hallucination_risk(entity: str, context: dict) -> dict:
    """Hallucination Risk Detector"""
    risk_score = 0.0
    reasons = []
    
    # Check 1: Entity trong knowledge base?
    if not is_in_knowledge_base(entity):
        risk_score += 0.4
        reasons.append("Entity not found")
    
    # Check 2: Context c√≥ contradictions?
    if has_internal_contradictions(context):
        risk_score += 0.3
        reasons.append("Internal contradictions")
    
    # Check 3: Confidence > evidence strength?
    evidence = calculate_evidence_strength(context)
    confidence = context.get('confidence', 0.5)
    
    if confidence > evidence * 1.2:
        risk_score += 0.3
        reasons.append(f"Confidence ({confidence:.2f}) >> Evidence ({evidence:.2f})")
    
    # Decision
    if risk_score > 0.6:
        return {
            'high_risk': True,
            'risk_score': risk_score,
            'recommendation': 'PROCEED_SAFELY_ADAPTER',  # request more info + constrain scope
            'reason': ' | '.join(reasons)
        }
    
    return {'high_risk': False, 'risk_score': risk_score}
```

**Usage Example**:

```markdown
Query: "Who is Michael Batkin?" (unknown entity)

Hallucination Check:
  Entity: "Michael Batkin"
  Knowledge base: NOT FOUND ‚ö†Ô∏è
  Confidence: 0.75 (high despite no evidence)
  Evidence strength: 0.1
  
  ‚Üí Risk: 0.7 (HIGH)
  ‚Üí Recommendation: Proceed-Safely (y√™u c·∫ßu th√™m th√¥ng tin v√† gi·ªõi h·∫°n ph·∫°m vi tr·∫£ l·ªùi an to√†n)
  
  Response: "M·ª©c tin c·∫≠y hi·ªán th·∫•p do thi·∫øu b·∫±ng ch·ª©ng. ƒê·ªÉ tr·∫£ l·ªùi an to√†n, vui l√≤ng cung c·∫•p th√¥ng tin/ngu·ªìn tham chi·∫øu c·ª• th·ªÉ ho·∫∑c gi·ªõi h·∫°n ph·∫°m vi c√¢u h·ªèi (v√≠ d·ª•: m·ªëc th·ªùi gian, t·ªï ch·ª©c, t√†i li·ªáu ch√≠nh th·ª©c). T√¥i s·∫Ω ti·∫øp t·ª•c v·ªõi ph·∫°m vi an to√†n ngay khi c√≥ d·ªØ li·ªáu b·ªï sung."
```

---

### 3. Uncertainty Expression

**Confidence Bands**:

| Range | Label | Communication |
|-------|-------|---------------|
| **0.95-1.0** | High confidence | "I'm confident that..." |
| **0.80-0.94** | Confident | "Based on [evidence], I conclude..." |
| **0.60-0.79** | Moderate | "It appears... though [uncertainty]" |
| **0.40-0.59** | Low confidence | "Multiple possibilities..." |
| **<0.40** | Very uncertain | "Insufficient evidence..." |

**Output Template**:

```markdown
Based on [evidence_sources], I [confidence_verb] that [conclusion].

[If confidence < 0.8]:
Key uncertainties:
- [Uncertainty 1]: [Why matters]
- [Uncertainty 2]: [Why matters]

Alternative explanations:
- [Alternative 1]: [Why plausible]

**Confidence**: [Band] ([score])
```

---

## Layer 4 Workflow

```
INPUT: Complex query from Layer 3
    ‚Üì
STEP 1: Run Layer 3 Reasoning
    ‚îú‚îÄ Generate hypotheses
    ‚îú‚îÄ Cross-verify (F + B + L)
    ‚îî‚îÄ Output: Preliminary conclusion
    ‚Üì
STEP 2: Self-Reflection
    ‚îú‚îÄ Assess quality (0-10)
    ‚îú‚îÄ Check logical gaps
    ‚îî‚îÄ Flag if quality < 7
    ‚Üì
STEP 3: Bias Detection
    ‚îú‚îÄ Scan 4 bias types
    ‚îú‚îÄ Document detected
    ‚îî‚îÄ Apply mitigation
    ‚Üì
STEP 4: Hallucination Check
    ‚îú‚îÄ For each entity: run detect
    ‚îú‚îÄ If high risk: request more info
    ‚îî‚îÄ Log all checks
    ‚Üì
STEP 5: Confidence Calibration
    ‚îú‚îÄ Compare historical accuracy
    ‚îú‚îÄ Apply confidence bands
    ‚îî‚îÄ Document uncertainties
    ‚Üì
OUTPUT: Meta-verified conclusion
    + Quality score
    + Detected biases (if any)
    + Calibrated confidence
    + Warnings (if any)
```

---

## Success Metrics

```yaml
Layer_4_Targets:
  hallucination_catch_rate: ">95%"
    # Known-false test cases, % refused
  
  bias_detection_rate: ">70%"
    # Adversarial queries, % flagged
  
  confidence_calibration_error: "<0.15 Brier"
    # (predicted - actual)^2 averaged
  
  reasoning_quality_score: ">8/10"
    # Human eval, 50-query sample
  
  latency_p95: "<120s"
    # Time from trigger to output
```

---

## Integration Points

**MCP Tool Enhancement**:

```typescript
// NEW Layer 4 parameters
interface SequentialThinkingParams {
  // ... existing ...
  
  // Layer 4 additions
  verification_mode?: 'meta';
  quality_score?: number;        // 0-10
  detected_biases?: string[];
  should_escalate?: boolean;
}
```

**Decision Logic**:

```typescript
function select_layer(query: Query): Layer {
  // High-stakes or quality issues ‚Üí Layer 4
  if (query.is_high_stakes || 
      confidence_dropped_below_0_6() ||
      backward_pass_failed() ||
      hypotheses_tied()) {
    return Layer.META;
  }
  // ... other layers ...
}
```

---

## üîó Related Rules

**Foundation**:

**Companion**:
- `rules/reasoning/18b-layer5-expert-reasoning.md` ‚Äî Layer 5 specs
- `rules/reasoning/18c-reasoning-verification.md` ‚Äî Cross-verification
- `rules/reasoning/18d-reasoning-integration.md` ‚Äî Integration guide

**Support**:
 

---

**Status**: Phase 1 Complete | **Size**: <12KB ‚úÖ  
**Implementation**: Week 1-2 | **Next**: Layer 5 (18b)
